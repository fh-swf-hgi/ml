{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excessive-output",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2021\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entscheidungsbäume und Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Funktionsweise von Entscheidungsbäumen und dem CART Algorithmus zu demonstrieren, verwenden wir ein einfaches Beispiel mit nur sehr wenigen Datenpunkten.\n",
    "Bei den in der Code-Zelle unten angegebenen Wetterdaten `temperatur` und `niederschlag` handelt es sich um Monatsmittelwerte.\n",
    "Der Datensatz hat also nur 12 Punkte.\n",
    "\n",
    "Als Klassen assoziieren wir zu den Monaten je eine Jahreszeit.\n",
    "Vereinfachend zählen wir die Monate Dezember bis Februar zum Winter, März bis Mai zum Frühling, u.s.w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temperatur = np.array([0.6,3.9,6.6,12.4,16.0,17.8,20.2,20.0,15.1,10.7,5.3,3.8])\n",
    "niederschlag = np.array([72,30,75,35,50,50,40,35,45,28,30,80])\n",
    "\n",
    "wetter_namen = ['Temperatur', 'Niederschlag']\n",
    "wetter_label_names = np.array(['Winter', 'Frühling', 'Sommer', 'Herbst'])\n",
    "wetter_label = np.array([0,0,1,1,1,2,2,2,3,3,3,0])\n",
    "wetter_tabelle = np.column_stack((temperatur,niederschlag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir aus dem Modul `sklearn.tree` die Klasse `DecisionTreeClassifier` verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "wetter_tree = DecisionTreeClassifier(max_depth=3)\n",
    "wetter_tree.fit(wetter_tabelle, wetter_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Vorhersagen von Entscheidungsbäumen sind intuitiv verständlich, vor allem, wenn man eine graphische Repräsentation des Baumes vorliegen hat.\n",
    "Wir können den Baum über die graphviz Bibliothek erzeugen.\n",
    "Eine Python-Implementierung von graphviz bietet die Bibliothek `pydot`.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} pydot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Baum anzuzeigen, exportieren wir zunächst das trainierte Modell in eine `dot`-Datei über die *sklearn* Funktion `export_graphviz`.\n",
    "Danach transformieren wir die `dot`-Datei mit der Funktion `write_png` in eine Grafik.\n",
    "Diese können wir dann mit der *pyplot* Funktion `imshow` anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import pydot\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "with open(\"wetter.dot\", 'w') as f:\n",
    "    f = export_graphviz(wetter_tree, \n",
    "                            feature_names=wetter_namen,\n",
    "                            class_names=wetter_label_names,\n",
    "                            filled=True, rounded=True,\n",
    "                            out_file=f)\n",
    "(graph,) = pydot.graph_from_dot_file(\"wetter.dot\")\n",
    "graph.write_png(\"wetter.png\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.imshow(img.imread('wetter.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `plot_decision_regions` aus der folgenden Code-Zelle stammt aus den Materialien zum Buch *Python Machine Learning* von Sebastian Raschka, ebenfalls ein sehr empfehlenswertes Buch zum Thema Maschinelles Lernen.\n",
    "Wir verwenden die Funktion zum Anzeigen der Entscheidungsgrenzen in einfachen Modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastian Raschka, 2015 (http://sebastianraschka.com)\n",
    "# Python Machine Learning - Code Examples\n",
    "#\n",
    "# Chapter 3 - A Tour of Machine Learning Classifiers Using Scikit-Learn\n",
    "#\n",
    "# S. Raschka. Python Machine Learning. Packt Publishing Ltd., 2015.\n",
    "# GitHub Repo: https://github.com/rasbt/python-machine-learning-book\n",
    "#\n",
    "# License: MIT\n",
    "# https://github.com/rasbt/python-machine-learning-book/blob/master/LICENSE.txt\n",
    "#\n",
    "#[https://github.com/rasbt/python-machine-learning-book/blob/master/code/optional-py-scripts/ch03.py]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_decision_regions(X, y, classifier, labelnames=None, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    if labelnames is None:\n",
    "        #len(labelnames)!=len(np.unique(y))?\n",
    "        labelnames = [i for i in np.unique(y)]\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=[cmap(idx)],\n",
    "                    marker=markers[idx], label=labelnames[cl])\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        # plot all samples\n",
    "        if not versiontuple(np.__version__) >= versiontuple('1.9.0'):\n",
    "            X_test, y_test = X[list(test_idx), :], y[list(test_idx)]\n",
    "            warnings.warn('Please update to NumPy 1.9.0 or newer')\n",
    "        else:\n",
    "            X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c='',\n",
    "                    alpha=1.0,\n",
    "                    linewidths=1,\n",
    "                    marker='o',\n",
    "s=55, label='test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir darstellen, wie die Monate nach den Kriterien *Temperatur* und *Niederschlag* den 4 Jahreszeiten zugeordnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(wetter_tabelle, wetter_label, classifier=wetter_tree, labelnames=wetter_label_names)\n",
    "plt.xlabel('Temperatur')\n",
    "plt.ylabel('Niederschlag')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was müssen Sie ändern, damit alle 12 Datenpunkte exakt eingeordnet werden? Was ist der Nachteil dieses Variante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der CART Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine eigene Implementierung des CART Algorithmus fügen wir vorab die Datenpunkte und Labes zu einer Matrix zusammen.\n",
    "Dies vereinfacht das Teilen der Datensätze (*split*) etwas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((wetter_tabelle,wetter_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die *Reiheit* einer Menge von Datenpunkten berechnen wir über das *Gini-Maß*.\n",
    "\n",
    "Die zugehörige Funktion `_gini` bestimmt zunächst die Menge der Labels.\n",
    "In unserer Matrix stehen die Labels in der letzten Spalte `data[:,-1]`.\n",
    "Da in einer Menge jeder Wert nur einmal enthalten sein kann sind in `c` nach der Operation je einmal die Werte 1-4 enthalten, mit denen die Jahreszeiten kodiert sind.\n",
    "\n",
    "Für jede Jahreszeit bestimmen wir die Anzahl an Datenpunkten die zu dieser Jahreszeit gehören.\n",
    "`d[:,-1]==cls` ist `True` für alle Zeilen der Matrix, die zu der Jahreszeit `cls` gehören.\n",
    "Mit der Operation `d[d[:,-1]==cls]` Rrduzieren wir die Matrix auf die Zeilen für die Jahreszeit `cls` und bestimmen dann mit `len` ihre Länge in Zeilen.\n",
    "Dieser Wert wird durch die Anzahl aller Datenpunkte `len(d)` geteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[tuple([data[:,-1]==1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gini(d):\n",
    "    if len(d)==0: return 1;\n",
    "    c = set(data[:,-1])\n",
    "    g=1\n",
    "    for cls in c:\n",
    "        p=len(d[d[:,-1]==cls])/len(d)\n",
    "        g -= p*p\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnen wir den *Gini-Wert* für die Ausgangsmenge, so erhalten wir das erwartete Resultat: $1-4\\left(\\frac{3}{12}\\right)^2=1-0.25=0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gini(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `_split` berechnet nun das Merkmal sowie die Bedingung an denen der Datensatz bestmöglich (d.h. mit maximalem Informationsgewinn) geteilt werden kann.\n",
    "\n",
    "Dazu läuft die Funktion in einer äußeren Schleife über alle Merkmale, also über alle Spaltennummern, bis auf die letzte (dort stehen die Labels).\n",
    "In jeder Spalte sortieren wir zunächst die Werte und bestimmen dann die möglichen Schnittpunkte.\n",
    "Diese Schnittpunkte liegen immer genau zwischen zwei aufeinander folgenden Werten.\n",
    "Mit `(last+i)/2` berechnen wir diesen Mittelwert und tragen in dann in die Liste `cuts` ein.\n",
    "\n",
    "Anschließend läuft die Funktion über alle möglichen Schnittpunkte und wertet jeweils die Kostenfunktion für den Informationsgewinn aus.\n",
    "\n",
    "Der global beste Wert und die das zugehörige Merkmal werden ermittelt und als Resultat zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split(data):\n",
    "    J_min=1\n",
    "    best_val=0\n",
    "    best_feature=0\n",
    "    N_data = len(data)\n",
    "    for feature in range(data.shape[1]-1):\n",
    "        cuts = []\n",
    "        f_sorted = np.sort(data[:,feature])\n",
    "        last = f_sorted[0]\n",
    "        for i in f_sorted[1:]:\n",
    "            cuts.append((last+i)/2)\n",
    "            last = i\n",
    "        for val in cuts:\n",
    "            true_set=data[data[:,feature]<=val]\n",
    "            false_set=data[data[:,feature]>val]\n",
    "            gini_t = _gini(true_set)\n",
    "            gini_f = _gini(false_set)\n",
    "            J = (gini_t*len(true_set)/N_data+gini_f*len(false_set)/N_data)\n",
    "            if J<=J_min:\n",
    "                J_min = J\n",
    "                best_val = val\n",
    "                best_feature=feature\n",
    "    \n",
    "    return best_feature,best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `_split` sollte nun in einer weiteren Funktion verwendet werden um den Entscheidungsbaum rekursiv aufzubauen.\n",
    "Für unser einfaches Beispiel ist es aber ebensogut möglich, die Schritte des Algorithmus \"per Hand\" durchzuspielen.\n",
    "\n",
    "Als ersten *Split* bestimmt die Funktion das Merkmal 0 (Temperatur) und den Wert 16.9.\n",
    "Die Kinder dieses Splits haben die *Gini-Werte* 0.67 (Temperatur kleiner oder gleich 16.9) und 0 (Temperatur größer 16.9).\n",
    "Damit wurde die Klasse der Sommermonate optimal abgedeckt.\n",
    "\n",
    "Da nur der linke Teilbaum einen *Gini-Wert* größer 0 hat, muss nur das linke Kind expandiert werden.\n",
    "Nun wird ebenfalls die Temperatur als Grundlage gewählt und die Wintermonate separiert.\n",
    "Im letzten Split werden die Frühjahr- und Herbstmonate über die Niederschlagsmenge getrennt.\n",
    "\n",
    "Der Entscheidungsbaum besitzt die selben Entscheidungsgrenzen, wie der von sklearn erzeugte.\n",
    "Allerdings besitzen die beiden Bäume eine andere Struktur, da die sklearn-Version zunächst die Wintermonate abspaltet.\n",
    "Dies hat ausschließlich mit der Reihenfolge der Berechnungen zu tun, das die *Gini-Werte* und Teilmengengrößen in beiden Fällen identisch sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature,best_val = _split(data)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set=data[data[:,best_feature]<best_val]\n",
    "right_set=data[data[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set), _gini(right_set)))\n",
    "\n",
    "best_feature,best_val = _split(left_set)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set1=left_set[left_set[:,best_feature]<best_val]\n",
    "right_set1=left_set[left_set[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set1), _gini(right_set1)))\n",
    "\n",
    "best_feature,best_val = _split(right_set1)\n",
    "print(\"Split column\", best_feature, \"at\", best_val)\n",
    "\n",
    "left_set2=right_set1[right_set1[:,best_feature]<best_val]\n",
    "right_set2=right_set1[right_set1[:,best_feature]>=best_val]\n",
    "print(\"Gini left: %.2f | Gini right %.2f\" % (_gini(left_set2), _gini(right_set2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreditvergabe mit Entscheidungsbäumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den folgenden Code-Zellen verwenden wir sklearn, um den aus dem letzten Arbeitsblatt bekannten Datensatz `kredit.csv` zu verarbeiten.\n",
    "Statt mit logistischer Regression wollen wir nun *gute* und *schlechte* Kredite über einen Entscheidungsbaum und später über *Random Forests* zu klassifizieren.\n",
    "\n",
    "Da die Schritte größtenteils selbsterklärend und aus vorherigen Beispielen bekannt sind, wird hier auf eine ausführliche Beschreibung verzichtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"kredit.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import pydot\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "#model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "scr = model.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Entscheidungsbaumes mit Tiefe 3 beträgt %.3f\" % scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kredite.dot\", 'w') as f:\n",
    "    f = export_graphviz(model,  \n",
    "                        feature_names=df.columns.values[1:],\n",
    "                        class_names=['schlecht','gut'],\n",
    "                        filled=True, rounded=True,\n",
    "                        out_file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graph,) = pydot.graph_from_dot_file(\"kredite.dot\")\n",
    "graph.write_png(\"kredite.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.imshow(img.imread('kredite.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.3, random_state=0)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "scr = model.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Entscheidungsbaumes beträgt %.3f\" % scr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun verwenden wir einen *Random Forrest* Klassifizierer mit einem Ensemble aus 100 Entscheidungsbaum-Instanzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forrest = RandomForestClassifier(n_estimators=100)\n",
    "forrest.fit(X_train,y_train)\n",
    "scr = forrest.score(X_test, y_test)\n",
    "print(\"Die Vorhersagegenauigkeit des Random Forests beträgt %.3f\" % scr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
