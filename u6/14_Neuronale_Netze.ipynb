{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "excessive-output",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2021\n",
    "Prof. Dr. Heiner Giefers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rückblick Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets , utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case\n",
    "m_c, n_c = 2, 4\n",
    "np.random.seed(0)\n",
    "X_c = np.random.randn(m_c, n_c)\n",
    "theta0_c, theta1_c = 0, np.random.randn(n_c, 1)\n",
    "y_c = np.random.randn(m_c, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Logistischen Regression haben wir das Gradientenverfahren benutzt, um die Parameter unseres Modells, einer linearen Funktion $$Z = f_{\\Theta}(x)=\\Theta_0+\\Theta_1x$$ transformiert durch die Aktivierungsfunktion $$\\hat y = h_{\\Theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$ schrittweise zu verbessern.\n",
    "Die Verbesserung, bzw. die Qualität des Modells, haben wir anhand der Kostenfunktion $$J_{\\Theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$ berechnet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "**Aufgabe: Schreibe eine Funktion $f$, die folgende Parameter erhält:**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält.\n",
    "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
    "\n",
    "**$f$ soll folgende lineare Funktion implementieren:** $$Z = f_{\\Theta}(X)=\\Theta_0+X\\Theta_1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a4ef91de334848c7ac4662886923232",
     "grade": false,
     "grade_id": "cell-a46e169efb324584",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f(X, theta0, theta1):\n",
    "    \"\"\"evaluates linear function.\n",
    "    Arguments:\n",
    "        X: value\n",
    "        theta0: Bias\n",
    "        theta1: Function slope\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e7f47bfdc7367ccbe7adba869ee0540",
     "grade": true,
     "grade_id": "cell-798b7b367b206c2f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "Z = f(X_c, theta0_c, theta1_c)\n",
    "#----------\n",
    "# f\n",
    "\n",
    "assert Z.shape == (m_c, 1), 'Use correctly sequenced matrix multiplication'\n",
    "assert np.isclose(Z[0], 3.38207), 'Expected 3.38207 but got %.5f' %Z[0]\n",
    "\n",
    "del Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Implementieren Sie die Perzepton-Fuinkion $h$. Die Funktion $h$ soll die gleichen Parameter wie $f$ erhalten und die Funktion $f$ intern aufrufen. Das Ergebnis von $f$ soll durch die Sigmoid-Aktivierungsfunktion transformiert werden.:** $$\\hat y = h_{\\Theta}(x) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b8e67d471948444e157b8f5a677a0b6",
     "grade": false,
     "grade_id": "cell-aed34bc92bba34fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def h(X, theta0, theta1):\n",
    "    \"\"\"returns the sigmoid of the linear function.\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta0: Bias\n",
    "        theta1: Function slope\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40893c252529355305847265c10395b4",
     "grade": true,
     "grade_id": "cell-3680cf0b32123e22",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "y_hat = h(X_c, theta0_c, theta1_c)\n",
    "#----------\n",
    "# h\n",
    "\n",
    "assert y_hat.shape == (m_c, 1)\n",
    "assert np.isclose(y_hat[0], 0.96713), 'Expected 0.96713 but got %.5f' %y_hat[0]\n",
    "\n",
    "del y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Berechnen Sie nun die Kostenfunktion. Schreiben Sie eine Funktion $J$, die folgende Parameter erhält:**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
    "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
    "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
    "\n",
    "**$J$ berechnet die folgende Kostenfunktion:** $$J_{\\Theta}(x)=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat y^{(i)}) + (1-y^{(i)})\\log(1- \\hat y^{(i)})]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf97ab2879007da3f8c2ac2574c078c0",
     "grade": false,
     "grade_id": "cell-2441e8b76f6c8b3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def J(X,theta0, theta1,y):\n",
    "    \"\"\"computes the Cross-entropy cost function\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta0: Bias\n",
    "        theta1: Function slope\n",
    "        y: True labels\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return J.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60205547ebb8495c0a7ed7adc1255849",
     "grade": true,
     "grade_id": "cell-6d386b0ed985b5a1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "cost = J(X_c,theta0_c,theta1_c,y_c)\n",
    "#----------\n",
    "# J\n",
    "\n",
    "assert cost.shape == (), 'Use correctly sequenced matrix multiplication'\n",
    "assert np.isclose(cost, 0.66739), 'Expected 0.66739but got %.5f' %cost\n",
    "\n",
    "del cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "**Schreibe für das Gradientenverfahren eine Funktion `grads`, die folgende Parameter erhält**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
    "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
    "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
    "\n",
    "**und den Fradienten $\\partial\\theta$ für die Parameter $\\theta$ berechnet** \n",
    "\n",
    "Dabei ist $\\partial\\theta_0$ ein Skalar der dem Gradienten des Bias Parameters entspricht: $$\\partial\\theta_0 = \\frac{1}{m} \\sum_{i=1}^m (\\hat y^{(i)}-y^{(i)})$$\n",
    "\n",
    "$\\partial\\theta_1$ ist ein Vektor der Dimension `(n, 1)` mit den Gradienten der anderen Parameter: $$ \\partial \\theta_1 = \\frac{1}{m}X^T(\\hat y-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c1b50500c83fa6a5a04644a3f60989f",
     "grade": false,
     "grade_id": "cell-467565ebb154795c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grads(X,theta0, theta1,y):\n",
    "    \"\"\"Calculates grads of the cost function with respect to the parameters.\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta0: Bias\n",
    "        theta1: Function slope\n",
    "        y: True labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return dtheta0, dtheta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec8bd1ce4f3515c4886ba5c631acc716",
     "grade": true,
     "grade_id": "cell-d5aaa7214b2dfbcd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "dt0, dt1 = grads(X_c,theta0_c,theta1_c,y_c)\n",
    "#----------\n",
    "# grads\n",
    "\n",
    "assert dt0.shape == (), 'theta0 should be a scaler'\n",
    "assert dt1.shape == theta1_c.shape\n",
    "assert np.isclose(dt1[0], 0.38273), 'Expected 0.38273 but got %.5f' %dt1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe: Schreiben Sie nun eine Funktion, die die Modellparameter aufgrund der berechneten Gradienten aktualisiert.Die Funktion `update`erhält die Parameter $\\theta$, die Gradienten $\\partial \\theta$ sowie die Lernrate $\\alpha$ und berechnet:\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\cdot \\partial \\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26a29c6b1b75695d6fe8f13dc43d02ec",
     "grade": false,
     "grade_id": "cell-9c9c665b5ef00354",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update(theta0, theta1,dtheta0, dtheta1, alpha):\n",
    "    \"\"\"updates parameters using gradient decent updating rule.\"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87aea3f4d59c25dfa95ddf5b27c608d4",
     "grade": true,
     "grade_id": "cell-628e4db830095cf8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "t0, t1 = update(theta0_c, theta1_c,dt0, dt1, 0.1)\n",
    "#----------\n",
    "# update\n",
    "\n",
    "assert t0.shape == (), 'theta0 should be a scaler'\n",
    "assert t1.shape == theta1_c.shape\n",
    "assert np.isclose(t1[0], -0.141491), 'Expected -0.141491 but got %.5f' %t1[0]\n",
    "del t0, t1, dt0, dt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir das iterative Gradientenverfahren programmieren.\n",
    "**Aufgabe: Schreiben Sie eine Funktion `gradient_descent`, die folgende Parameter erhät:**\n",
    "1. Die Matrix $X \\in \\mathbb{R}^{m\\times{}n}$, die die Datenpunkte des Trainigsdatensatzes enthält. .\n",
    "2. Die Parameter $\\Theta$. Dabei ist der Bias Parameter $\\Theta_0$ ein skalarer Wert, $\\Theta_1$ hingegen ein Vektor aus $\\mathbb{R}^n$, in Numpy also mit der Dimension `(n, 1)`.\n",
    "3. Die Label $y$ in der Größe des Datensatzes `(m, 1)`.\n",
    "4. Die Lernrate $\\alpha$.\n",
    "5. Die Anzahl der Iterationen.\n",
    "**Die Funktion soll die Trainierten Modellparameter $\\theta$ zurückgeben.**\n",
    "\n",
    "*Hinweis*: Berechnen Sie Kosten mit der Funktion `J` und hängen Sie diese Kosten nach jedem BErechnungsschritt and die Liste `cost` an --> Berechnen Sie die Gradienten mit der Funktion `grads` --> Verwenden Sie diese Gradienten um die Parameter mit der Funktion `optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6685f2b92b31490129b96fdf3f61632",
     "grade": false,
     "grade_id": "cell-c950a15eb73cda45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_decent(X, theta0, theta1, y, alpha=0.1, iterations=400):\n",
    "    \"\"\"performs gradient decent optimization.\n",
    "    Arguments:\n",
    "        X: Data\n",
    "        theta0: Bias\n",
    "        theta1: Function slope\n",
    "        y: True labels\n",
    "        alpha(default=0.1): Learning rate\n",
    "        iterations(default=400): number of updating iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    return theta0, theta1, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6b26ea20d41f5ece7f61230e1ec706c",
     "grade": true,
     "grade_id": "cell-9e057a811aa97078",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "t0, t1, costs = gradient_decent(X_c, theta0_c, theta1_c, y_c)\n",
    "#----------\n",
    "# gradient_decent\n",
    "\n",
    "assert len(costs) == 400, 'Make sure to calculate and append the cost in every iteration.'\n",
    "assert np.isclose(t1[3], 1.05186), 'Expected 1.05186 but got %.5f' %t1[3]\n",
    "\n",
    "del t0, t1, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainieren Sie das Modell\n",
    "\n",
    "Wir habe nun alle Funktionen um unser Perzeptron für einen *echten* Datensatz zu einzusetzen.\n",
    "Wir verwenden hier den Brustkrebs-Datensatz aus Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,test_size=0.3)\n",
    "\n",
    "# preprocessing\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = np.expand_dims(y_train, 1)\n",
    "y_test = np.expand_dims(y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing parameters\n",
    "theta0 = 0.\n",
    "theta1 = np.zeros((len(X_train[0]), 1))\n",
    "\n",
    "#training the model\n",
    "theta0, theta1, costs = gradient_decent(X_train, theta0, theta1, y_train.reshape(-1, 1), alpha = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(costs)),costs[1:], \"x-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring performance\n",
    "y_pred = (h(X_test,theta0, theta1) >= 0.5)*1\n",
    "acc = 100-np.sum(np.abs(y_pred-y_test))*100/len(y_test)\n",
    "\n",
    "print(\"Die classifcation accuracy ist: \",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher haben wir ein Perzeptron betrachtet, also eine Art Neuronales Netz mit nur einer Schicht. Die gleichen Methoden können wir aber auch für mehrschichtige Neuronale Netze verwenden.\n",
    "\n",
    "# Neurale Netze\n",
    "\n",
    "(*wird noch übersetzt*)\n",
    "\n",
    "Similarly, we will generalize the same structure to write the functions for forward and backward propagation and optimization but with one difference is that the input-out dimensions are a little more flexable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "**First we build a function that takes in:**\n",
    "1. The layer input data matrix $A$ where each column is a single datapoint and the number of columns is the size of the dataset ($A = X$ for the first layer and $A = \\hat y$ for the last layer).\n",
    "2. The parameters $\\Theta$. Note: from now on we will refer to $\\Theta_0$ as $b$ and $\\Theta_1$ as $W$ where $b$ is a vector value representing the bias for each node and $W$ is a matrix having the dimensions of the current layer and the previous layer.\n",
    "\n",
    "**and returns the linear function:** $$Z = f_{\\Theta}(A)= b + AW$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(A, b, W):\n",
    "    \"\"\"evaluates linear function.\n",
    "    Arguments:\n",
    "        A: Layer input data\n",
    "        b: Biases vector\n",
    "        W: Weights matrix\n",
    "    \"\"\"\n",
    "    Z = b + A@W\n",
    "    cache = (A, b, W) # keeping cache is important for backpropagation\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce the nonlinearity, **we build the sigmoid activation function that takes in the same arguments from before and returns the activation function:** $$A_{next}= h_{\\Theta}(A) = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(A, b, W):\n",
    "    \"\"\"computes the sigmoid of the linear function.\n",
    "    Arguments:\n",
    "        A: Layer input data\n",
    "        b: Biases vector\n",
    "        W: Weights matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, Zcache = f(A, b, W)\n",
    "    A = 1.0 / (1.0+np.exp(-Z))\n",
    "    cache = (Zcache, Z) # keeping cache is important for backpropagation\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dimensions of the network is determined by those of parameters, therefore it is convenient to write a function to initalize the parameters and dictate network architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(dims):\n",
    "    \"\"\"returns initial weights and biases.\"\"\"\n",
    "    #dims = [X,L1,L2,...,y]\n",
    "    b = []\n",
    "    W = []\n",
    "    \n",
    "    for l in range(1, len(dims)):\n",
    "        b.append(np.zeros((1, dims[l])))\n",
    "        W.append(np.random.randn(dims[l-1], dims[l])*0.01) # to break symmetry, it's good to have non-zero weights \n",
    "        \n",
    "    return b, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the building blocks ready, **we can create a function that repeats the same calculation over multiple layers from input layer $X$ to output layer $y$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, b, W):\n",
    "    \"\"\"Performs a forward propagation through all layers.\n",
    "    returns the output layer and cache of intermidiates.\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    for l in range(len(b)):\n",
    "        A_prev = A\n",
    "        A, cache = h(A_prev, b[l], W[l])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate the cost function. **We build a function that takes in:**\n",
    "1. The predicted labels through forward propagation $\\hat{y}$ in the size of the dataset (1, m).\n",
    "3. The true labels  $y$ in the size of the dataset (1, m).\n",
    "\n",
    "**and returns the cost function:** $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1- \\hat{y}^{(i)})]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(y_pred, y):\n",
    "    \"\"\"evaluates Cross-entropy cost function.\"\"\"\n",
    "    J = -(y.T@np.log(y_pred) + ((1.0-y.T)@np.log(1.0-y_pred)))/len(y)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "For gradients . **We build a function that takes in:**\n",
    "1. The gradient of cost wrt linear function ($\\partial Z$)\n",
    "2. The cache of $f$ collected during forward propagation.\n",
    "\n",
    "**and returns the grads for the parameters $\\partial b, \\partial W, \\partial A_{prev}$.** \n",
    "\n",
    "Note that $\\partial b$ is a vector representing the bias grads: $$\\partial b = \\frac{1}{m} \\sum_{i=1}^m (\\partial z^{(i)})$$\n",
    "\n",
    "\n",
    "and $\\partial W$ is a matrix having the layer deminsions: $$ \\partial W = \\frac{1}{m}A_{prev}^T\\partial Z$$\n",
    "\n",
    "and $\\partial A_{prev}$ is gradient of cost wrt activation function $A$ of the previous layer $$ \\partial A_{prev} = \\frac{1}{m}\\partial ZW^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_back(dZ, cache):\n",
    "    \"\"\"calculates grads w.r.t. parameters.\"\"\"\n",
    "    \n",
    "    A_prev, b, W = cache\n",
    "    m = len(A_prev[0])\n",
    "    \n",
    "    db = np.sum(dZ, axis=0, keepdims=True)/m\n",
    "    dW = (A_prev.T@dZ)/m\n",
    "    dA_prev = (dZ@W.T)/m\n",
    "    \n",
    "    return dA_prev, db, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gradients . **We build a function that takes in:**\n",
    "1. The gradient of cost wrt activation function $\\partial A$\n",
    "2. The cache of $h$ collected during forward propagation.\n",
    "\n",
    "**and returns the grads for the parameters $\\partial b, \\partial W, \\partial A_{prev}$.**\n",
    "\n",
    "Note that $\\partial Z$ is a matrix having the layer deminsions: $$\\partial Z = \\partial A \\cdot \\sigma (Z) (1-\\sigma (Z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_back(dA, cache):\n",
    "    \"\"\"calculates grads w.r.t. linear function.\"\"\"\n",
    "    \n",
    "    Zcache, Z = cache\n",
    "\n",
    "    sigma = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * sigma * (1-sigma)\n",
    "    dA_prev, db, dW = f_back(dZ, Zcache)\n",
    "    \n",
    "    return dA_prev, db, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now iterate these function backward over all layers and gather the gradients along the way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads(y_hat, y, caches):\n",
    "    \"\"\"performs backprobagation through all layers to calculate grads of all parameters.\"\"\"\n",
    "    \n",
    "    db = []\n",
    "    dW = []\n",
    "    \n",
    "    dA = -(y/y_hat - (1-y)/(1-y_hat)) # derivative of cost with respect to y_hat\n",
    "    \n",
    "    for l in reversed(range(len(caches))):\n",
    "        dA, db_, dW_ = h_back(dA, caches[l])\n",
    "\n",
    "        db.insert(0, db_)\n",
    "        dW.insert(0, dW_)\n",
    "        \n",
    "    return db, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For gradient decent optimization. We write a function that updates the parameters, it recieves:\n",
    "1. parameters for the entire network ($b, W$).\n",
    "2. grads of the entire network ($\\partial b, \\partial W$).\n",
    "3. learning rate $\\alpha$.\n",
    "\n",
    "and returns the updated set of parameters ($b, W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim(parameters, grads, alpha):\n",
    "    \"\"\"updates parameters using gradient decent updating rule.\"\"\"\n",
    "    \n",
    "    b, W = parameters\n",
    "    db, dW = grads\n",
    "    \n",
    "    for l in range(len(b)):\n",
    "        b[l] = b[l] - alpha*db[l]\n",
    "        W[l] = W[l] - alpha*dW[l]\n",
    "        \n",
    "    return b, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "Now that we have set up the network, let's create a model and train it on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing MNIST from tensorflow dataset liabrary\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out an random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 10, figsize=(18,3))\n",
    "\n",
    "for i in range(10):\n",
    "    ax[i].set_title('Image label: %d' %i)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(X_train[y_train == i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "X_train = X_train.reshape(-1, 28*28)/255\n",
    "X_test = X_test.reshape(-1, 28*28)/255\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y, layers_dims, alpha = 0.01, epoch = 20, batch_size = 400):\n",
    "    \"\"\"An MLP that optimizes Cross-entropy cost function using gradient decent.\n",
    "    X: Input data\n",
    "    y: Target labels\n",
    "    layers_dims: A list of layers' dimensions\n",
    "    alpha(default = 0.01): Learning rate\n",
    "    epoch(default = 20): Number of epochs\n",
    "    batch_size(default = 400): Size of minibatches\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    parameters = init(layers_dims)\n",
    "    b, W = parameters\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        \n",
    "        for batch in range(0,len(y),batch_size):\n",
    "            y_hat, caches = forward(X[batch:batch+batch_size], b, W)\n",
    "            cost = J(y_hat, y[batch:batch+batch_size])\n",
    "            gradss = grads(y_hat, y[batch:batch+batch_size], caches)\n",
    "            parameters = optim(parameters, gradss, alpha)\n",
    "            b, W = parameters\n",
    "\n",
    "        costs.append(cost.item(0))\n",
    "        print(f'Epoch {i}: {cost.item(0)}')\n",
    "            \n",
    "    plt.plot(range(1,len(costs)),costs[1:], \"x-\")\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"predict labels using learned parameters.\"\"\"\n",
    "    \n",
    "    b, W = parameters\n",
    "    y_pred, _ = forward(X, b, W)\n",
    "    \n",
    "    return (y_pred>=0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_wieghts_0 = model(X_train, y_train, [28*28, 10], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_wieghts_1 = model(X_train, y_train, [28*28, 20, 10], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = predict(X_test, trained_wieghts_0)\n",
    "acc_0 = 100-np.sum(np.abs(y_pred_0-y_test))*100/len(y_test)\n",
    "\n",
    "y_pred_1 = predict(X_test, trained_wieghts_1)\n",
    "acc_1 = 100-np.sum(np.abs(y_pred_1-y_test))*100/len(y_test)\n",
    "\n",
    "acc_0, acc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a classified random sample from the testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_i = np.random.randint(10000, size=8)\n",
    "fig, ax = plt.subplots(1, 8, figsize=(18,3))\n",
    "for i, j in enumerate(img_i):\n",
    "    ax[i].set_title('True: %d\\nPredicted: %d' %(np.argmax(y_test[j]), np.argmax(y_pred_1[j])))\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(X_test[j].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we learn how to build neural nets more easily in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze mit Keras\n",
    "\n",
    "In dieser Aufgabenblatt geht es darum, ein einfachen Künstliches Neuronales Netz für ein Regressionsproblem mit der Keras API zu implementieren.\n",
    "\n",
    "Wir verwenden den *Boston Housing* Datensatz, der als Standard-Anwendungsbeispiel über die Keras API heruntergeladen werden kann.\n",
    "\n",
    "Der Datensatz beschreibt die Wohnverhältnisse in verschieden Gebieten um Boston in den 1970er Jahren.\n",
    "Er enthält 506 Einträge mit jeweils 13 numerischen Merkmalen, die Zielvariable beschreibt den mittleren Wert der Häuserpreise in dem jeweiligen Bezirk (in Tausend USD).\n",
    "\n",
    "Gute Modelle sollten Vorhersagen mit einer mittlere quadratische Abweichung (*mean squared error*, MSE) unterhalb 20 (Tausend USD) treffen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst binden wir die wichtigsten Bibliotheken ein und laden den Datensatz mit der Funktion `boston_housing.load_data()` herunter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train.mean(axis=0)\n",
    "X_train -= mean\n",
    "std = X_train.std(axis=0)\n",
    "X_train /= std\n",
    "\n",
    "X_test -= mean\n",
    "X_test /= std\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Modellparameter zusammengestellt sind, muss die Eigentliche Modellfunktion erstellt werden.\n",
    "In Tensorflow wird damit der Graph des Modells erzeugt.\n",
    "Die `compile`-Funktion kann mit verschiedenen Parametern angepasst werden:\n",
    "- `optimizer` legt die Art (den Algorithmus) des Gradientenverfahrens fest. (https://keras.io/optimizers/)\n",
    "- `loss` bestimmt die Art der Kostenfunktion (https://keras.io/losses/)\n",
    "- `metrics` legt fest, welche Qualitätsmerkmale beim Trainieren überprüft werden sollen (https://keras.io/metrics/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='mse',\n",
    "    metrics=['mae','mape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Ergebnisse zu visulaisieren, legen wir ein Log-Verzeichnis pro Lauf des Verfahrens an, in das wir die Informationen der `fit`-Funktion schreiben.\n",
    "Damit können wir später die Ergebnisse der einzelnen Trainingsläufe vergeleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "newlog = '.\\\\keras_data\\\\run_' + time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=newlog, histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun rufen wir die `fit`-Funktion auf, um die Modellparameter zu lernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    callbacks=[tbCallBack]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Training verwenden wir die Testdaten, um unser trainiertes Modell zu evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_score, mae_score, mape_score = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Mittlwert der Fehlerquadrate: \", mse_score)\n",
    "print(\"Mittlerer absoluter Fehler: \", mae_score)\n",
    "print(\"Mittlerer absoluter Fehler (in Prozent): \", mape_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Trainierte Modell schätzt die Häuserpreise vermutlich nicht sonderlich gut.\n",
    "Dies liegt an der sehr unglücklichen Wahl der (Hyper-)Parameter.\n",
    "Versuchen Sie die Parameter sinnvoll anzupassen, sodass sich die Qualität der Schaätzfunktion verbessert.\n",
    "\n",
    "Um die Log-Dateien sowie den Modellgraphnen zu visualisieren, können die Tensorboard verwenden.\n",
    "Rufen Sie daszu (im Verzeichnis der *.ipynb* Datei) das Kommando `tensorboard --logdir=./keras_data` auf.\n",
    "Wenn Tensorboard gestartet ist, erreichen Sie die Web-Oberfläche unter der URL `localhost:8008`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.get_weights():\n",
    "    print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden nun in einem zweiten Beispiel Keras mit dem MNIST Datensatz:\n",
    "\n",
    "### Beschaffen der Daten und Vorverarbeiten\n",
    "\n",
    "1. Daten mit `datasets.mnist.load_data()` laden\n",
    "2. Bilder in eindimensionale Arrays umwandeln mit `reshape`. Teilen durch 255 um zu Normalisieren.\n",
    "3. One-Hot Encoding der Labels mit `utils.to_categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59125ac0bbf8ef72b5b8cd84b92692e3",
     "grade": false,
     "grade_id": "cell-a1780d72ee450607",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "(X_train, y_train), (X_test, y_test) = (None, None), (None, None)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d00654da7a9e5edd5e624f930719d7",
     "grade": true,
     "grade_id": "cell-b51934e76e8f110e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert X_train.ptp() == X_test.ptp() == 1, 'Data is not nomalized'\n",
    "assert X_train.shape == (60000, 784)\n",
    "assert X_test.shape == (10000, 784)\n",
    "assert y_train.shape == (60000, 10), 'Make sure to one hot encode train labels'\n",
    "assert y_test.shape == (10000, 10), 'Make sure to one hot encode test labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Modell aufstellen\n",
    "Erstellen Sie ein Modell mit geeigneten Hyperparametern:\n",
    "\n",
    "*Hinweise:*\n",
    "- Verwenden Sie die `softmax` Aktivierungsfunktion um die Ausgabeschicht in eine Wahrscheinlichkeitsfunktion umzuwandeln.\n",
    "- `input_shape` ist ein Vektor der Dimension `(784,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a292861da4c635a3f487a0b67e72cc72",
     "grade": false,
     "grade_id": "cell-ee3d61912367ba52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mnist_model = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26af5c2c9dd524c2e382e768a3539ca6",
     "grade": true,
     "grade_id": "cell-9f1b71a062361e10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert type(mnist_model) == models.Sequential\n",
    "assert mnist_model.built, 'The model is not built'\n",
    "assert len(mnist_model.layers) > 2, 'You should have at least one hidden layer'\n",
    "assert mnist_model.input_shape[1:] == X_train[0].shape, 'Input must match the features of the image'\n",
    "assert mnist_model.output_shape[1] == 10, 'Output must match the number of classes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell Kompilieren\n",
    "VErwenden Sie `compile` um den `optimizer`, die `loss` Funktion und die `metrics` festzulegen.\n",
    "\n",
    "*Hinweise:* \n",
    "- Da wir `softmax` verwenden, wählen wir die Kostenfunktion `categorical_crossentropy`.\n",
    "- `accuracy` ist an dieser Stelle eine gute Metrik für die Klassifikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48240bfab7925608c3fabb416a94d5a6",
     "grade": false,
     "grade_id": "cell-f41104f1f759e66f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fee02e795f4f9933aaa5a2326205173",
     "grade": true,
     "grade_id": "cell-6e6195d6ba6fa627",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert 'accuracy' in mnist_model.metrics_names\n",
    "assert mnist_model.loss\n",
    "assert mnist_model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Trainieren Sie das Modell mit der `fit` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "181b802c7ce6c3d691ddfe7b4935db10",
     "grade": false,
     "grade_id": "cell-9c7096937405635f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a6ad5e3cf54065ef768acfc7e501ac6",
     "grade": true,
     "grade_id": "cell-fc3dc3a7dd085783",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell\n",
    "#----------\n",
    "\n",
    "assert len(mnist_model.history.epoch) >= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testen\n",
    "Evaluieren Sie das Modell mit der `evaluate` Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdf6fcb45a60772921d356eede59d0c9",
     "grade": false,
     "grade_id": "cell-daf429a21c6e809b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "loss, acc = [None]*2\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(\"loss: %.4f - accuracy: %.4f \" %(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_i = np.random.randint(10000, size=8)\n",
    "fig, ax = plt.subplots(1, 8, figsize=(18,3))\n",
    "for i, j in enumerate(img_i):\n",
    "    ax[i].set_title('True: %d\\nPredicted: %d' %(np.argmax(y_test[j]), mnist_model.predict_classes(X_test[j].reshape(1, -1))))\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(X_test[j].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referenzen:\n",
    "[1] M. Berthold, C. Borgelt, F. Höppner and F. Klawonn, Guide to Intelligent Data Analysis, London: Springer-Verlag, 2010.  \n",
    "[2] A. Ng, Machine Learning Yearning, deeplearning.ai, 2018.  \n",
    "\n",
    "**Tipp:** Schauen Sie sich die Playlist von *3Blue1Brown* über Neuronale Netze auf [Youtube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
